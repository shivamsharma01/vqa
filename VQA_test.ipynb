{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LOdNIeUR5XZm"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "#from utilities import text_helper\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from data_loader import get_loader\n",
    "from models import VqaModel, SANModel\n",
    "import warnings \n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#from resize_images import resize_image\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_str_list(fname):\n",
    "    with open(fname) as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [l.strip() for l in lines]\n",
    "    return lines\n",
    "\n",
    "\n",
    "qst_vocab = load_str_list(\"datasets/vocab_questions.txt\")\n",
    "ans_vocab = load_str_list(\"datasets/vocab_answers.txt\")\n",
    "word2idx_dict = {w:n_w for n_w, w in enumerate(qst_vocab)}\n",
    "unk2idx = word2idx_dict['<unk>'] if '<unk>' in word2idx_dict else None\n",
    "vocab_size = len(qst_vocab)\n",
    "\n",
    "def load_image(image_path, transform=None):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = image.resize([224, 224], Image.LANCZOS)\n",
    "    \n",
    "    if transform is not None:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def visualizeAttention(model, img, layer):\n",
    "    m = nn.Upsample(size=(224,224), mode='bilinear')\n",
    "    pi = model.attn_features[layer].squeeze()\n",
    "    print(pi.size())\n",
    "    pi = pi.view(14,14)\n",
    "    attn = m(pi)\n",
    "    \n",
    "    image = image.squeeze(0)\n",
    "    img = torch.numpy(img)\n",
    "    attn  = torch.numpy(attn)\n",
    "#     print(image.shape, attn.shape)\n",
    "    ## Visualization yet to be completed\n",
    "    \n",
    "\n",
    "def word2idx(w):\n",
    "    if w in word2idx_dict:\n",
    "        return word2idx_dict[w]\n",
    "    elif unk2idx is not None:\n",
    "         return unk2idx\n",
    " \n",
    "    else:\n",
    "        raise ValueError('word %s not in dictionary (while dictionary does not contain <unk>)' % w)\n",
    "        \n",
    "def main(args):\n",
    "     \n",
    " \n",
    "   \n",
    "    image = cv2.imread(args.image_path)\n",
    "    image = cv2.resize(image, dsize=(224,224), interpolation = cv2.INTER_AREA)\n",
    "    image = torch.from_numpy(image).float()\n",
    "    image = image.to(device)\n",
    "    image = image.unsqueeze(dim=0)\n",
    "    image = image.view(1,3,224,224)\n",
    "    \n",
    "    max_qst_length=30\n",
    "    \n",
    "    question = args.question\n",
    "    q_list = list(question.split(\" \"))\n",
    "#     print(q_list)\n",
    "    \n",
    "    idx = 'valid'\n",
    "    qst2idc = np.array([word2idx('<pad>')] * max_qst_length)  # padded with '<pad>' in 'ans_vocab'\n",
    "    qst2idc[:len(q_list)] = [word2idx(w) for w in q_list]\n",
    "\n",
    "    question = qst2idc\n",
    "    question = torch.from_numpy(question).long()\n",
    "    \n",
    "    question = question.to(device)\n",
    "    question = question.unsqueeze(dim=0)\n",
    "    model = torch.load(args.saved_model)\n",
    "    model = model.to(device)\n",
    "    #torch.cuda.empty_cache()\n",
    "    model.eval()\n",
    "    output = model(image, question)\n",
    "      \n",
    "#     Visualization yet to be implemented\n",
    "#     if model.__class__.__name__ == \"SANModel\":\n",
    "#         print(model.attn_features[0].size())\n",
    "#          visualizeAttention(model, image, layer=0)\n",
    "    predicts = torch.softmax(output, 1)\n",
    "    probs, indices = torch.topk(predicts, k=5, dim=1)\n",
    "    probs = probs.squeeze()\n",
    "    indices = indices.squeeze()\n",
    "    print(\"predicted - probabilty\")\n",
    "    for i in range(5):\n",
    "#         print(probs.size(), indices.size())\n",
    "#         print(ans_vocab[indices[1].item()],probs[1].item())\n",
    "        print(\"'{}' - {:.4f}\".format(ans_vocab[indices[i].item()], probs[i].item()))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "   \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--image_path', type = str, required=True)\n",
    "    parser.add_argument('--question', type = str, required=True)\n",
    "    parser.add_argument('--saved_model', type = str, required=True)\n",
    "       \n",
    "    args = parser.parse_args()\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "VQA-test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
