{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"(Preprocess images and basic code)Main  DL - Base line 1.ipynb","provenance":[{"file_id":"10XYNJVKR3DXz3liAdFxuFDZS3sym62ej","timestamp":1616061274667},{"file_id":"1Fm7F7_9KJnr06nKn9Em6gRgKxaZ8nD_i","timestamp":1616012656198}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r3INTlYtd7ej","executionInfo":{"status":"ok","timestamp":1616218188438,"user_tz":-330,"elapsed":1477,"user":{"displayName":"Akanksha Shrimal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTgb81ryS4Z8HL8jjPWL6oDEg75JbHezBN4Gnk=s64","userId":"15584575180220244618"}},"outputId":"cd6de2f4-4ea5-491d-ced8-ac2a58abb0fc"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V94ltqwk9bIO"},"source":["# !zip -r /content/drive/MyDrive/SUB/PROJECTS/Dl/processed_images/myZipData.zip /content/drive/MyDrive/SUB/PROJECTS/Dl/processed_images/\n","# !zip -r /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Annotated_images/a.zip /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Annotated_images/"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T9t5_n6UcYnW"},"source":["# Import Statements "]},{"cell_type":"code","metadata":{"id":"QOjqqpJHcqGU"},"source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pickle\n","import pandas as pd \n","import matplotlib.pyplot as plt\n","from matplotlib.pyplot import figure \n","import json\n","import datetime\n","import copy\n","from PIL import Image as im \n","import joblib\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JhMojvOe-w1-"},"source":["import torch\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms, models\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sb3hnDG6a1eY"},"source":["import cv2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TX5_vDVWcYvL"},"source":["# Loading and Saving functions "]},{"cell_type":"code","metadata":{"id":"VdVP3k-7cqzu"},"source":["# Saving and Loading models using joblib \n","def save(filename, obj):\n","  with open(filename, 'wb') as handle:\n","      joblib.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","def load(filename):\n","  with open(filename, 'rb') as handle:\n","      return joblib.load(filename)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pgBXoOfKcY2d"},"source":["# Preprocess dataset "]},{"cell_type":"markdown","metadata":{"id":"BRJ6pAF6DkbZ"},"source":["## Download data "]},{"cell_type":"code","metadata":{"id":"hMONmhedcrXY"},"source":["# # Dataset link https://visualqa.org/download.html\n","# # Balanced VQA Images Annotated \n","# # !wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Train_mscoco.zip -P /content/drive/MyDrive/SUB/PROJECTS/Dl/data\n","# # !wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Val_mscoco.zip -P /content/drive/MyDrive/SUB/PROJECTS/Dl/data\n","\n","# # VQA input Questions \n","# # !wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Train_mscoco.zip -P  /content/drive/MyDrive/SUB/PROJECTS/Dl/data\n","# !wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Val_mscoco.zip -P /content/drive/MyDrive/SUB/PROJECTS/Dl/data\n","# !wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Test_mscoco.zip -P /content/drive/MyDrive/SUB/PROJECTS/Dl/data \n","\n","# # VQA Input Images \n","# !wget http://images.cocodataset.org/zips/train2014.zip -P /content/drive/MyDrive/SUB/PROJECTS/Dl/data \n","# !wget http://images.cocodataset.org/zips/val2014.zip -P /content/drive/MyDrive/SUB/PROJECTS/Dl/data \n","# !wget http://images.cocodataset.org/zips/test2015.zip -P /content/drive/MyDrive/SUB/PROJECTS/Dl/data \n","\n","# # VQA Complementary Pair List \n","# !wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Complementary_Pairs_Train_mscoco.zip -P /content/drive/MyDrive/SUB/PROJECTS/Dl/data\n","# !wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Complementary_Pairs_Val_mscoco.zip -P /content/drive/MyDrive/SUB/PROJECTS/Dl/data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sE8fl9YbZTRn"},"source":["# # Dataset link https://visualqa.org/download.html\n","# # Balanced VQA Images Annotated \n","# !wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Train_mscoco.zip -P /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Annotated_images\n","# !wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Val_mscoco.zip -P /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Annotated_images\n","\n","# !unzip /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Annotated_images/v2_Annotations_Train_mscoco.zip -d /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Annotated_images\n","# !unzip /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Annotated_images/v2_Annotations_Val_mscoco.zip -d /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Annotated_images\n","\n","# # VQA input Questions \n","# !wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Train_mscoco.zip -P  /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Questions\n","# !wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Val_mscoco.zip -P /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Questions\n","# !wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Test_mscoco.zip -P /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Questions\n","\n","# !unzip /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Questions/v2_Questions_Train_mscoco.zip -d /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Questions\n","# !unzip /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Questions/v2_Questions_Val_mscoco.zip -d /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Questions\n","# !unzip /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Questions/v2_Questions_Test_mscoco.zip -d /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Questions\n","\n","\n","# # VQA Input Images \n","# !wget http://images.cocodataset.org/zips/train2014.zip -P /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Images\n","# !unzip /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Images/train2014.zip -d /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Images\n","\n","# !wget http://images.cocodataset.org/zips/val2014.zip -P /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Images\n","# !unzip /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Images/val2014.zip -d /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Images\n","\n","# !wget http://images.cocodataset.org/zips/test2015.zip -P /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Images\n","# !unzip /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Images/test2015.zip -d /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Images\n","\n","# # VQA Complementary Pair List \n","# !wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Complementary_Pairs_Train_mscoco.zip -P /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Pair_list\n","# !wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Complementary_Pairs_Val_mscoco.zip -P /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Pair_list\n","\n","\n","# !unzip /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Pair_list/v2_Complementary_Pairs_Train_mscoco.zip -d /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Pair_list\n","# !unzip /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Pair_list/v2_Complementary_Pairs_Val_mscoco.zip -d /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Pair_list/"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hr5LBZh-2_-J"},"source":["## Pre-process Images "]},{"cell_type":"code","metadata":{"id":"P8bikO0M3jdV"},"source":["# images_path = \"/content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Images/\"\n","# save_path = \"/content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Resized_images2/\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6NBgnCYdU28d"},"source":["# # # Resize Images \n","# # from PIL import Image as im \n","\n","# def read_images(datapath, savepath):\n","#   # folder_arr = [\"train2014/\", \"test2015/\", \"val2014/\"]\n","#   folder_arr = [\"val2014/\"]\n","#   count = 0\n","\n","#   num1 = 224\n","#   num2 = 224\n","#   temp = 29288\n","\n","#   for f in folder_arr:\n","#     path = os.path.join(datapath , f)\n","#     for img in os.listdir(path):\n","#       if count < temp:\n","#         count = count + 1\n","#         continue\n","#       count = count + 1\n","#       name = img\n","#       print(name, count)\n","#       img = cv2.imread(os.path.join(path,img))\n","#       new_img = cv2.resize(img, (num2, num1))\n","#       # print(new_img)\n","#       data = im.fromarray(new_img)\n","#       data.save(savepath + f + name) \n","\n","#     print(f + \"---------------- done-------------\")  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OyTJ5hje3Dlp"},"source":["# read_images(images_path, save_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N_B0GnDoIIPv"},"source":["# Dividing images - Train 40k , Val 16k , Test 16k (Approximately)\n","# import shutil \n","# path2 = \"/content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/v_data/val/\"\n","# path1 = \"/content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/train2014/\"\n","# path3 = \"/content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/te_data/test/\"\n","# path4 = \"/content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/tr_data/train/\"\n","\n","# imgs = os.listdir(path3)\n","# size = len(imgs)\n","# print(size)\n","# np.random.shuffle(imgs)\n","# # other = imgs[:int(1*size)]\n","# other = imgs[:2505]\n","# print(len(other))\n","\n","# for img in other:\n","#   # shutil.move(path1 + img, path4) \n","\n","\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r3xVUTA6cZEs"},"source":["# Utility Functions "]},{"cell_type":"markdown","metadata":{"id":"1lRpLXRrzYcc"},"source":["## Preprocess questions and answers file  "]},{"cell_type":"code","metadata":{"id":"fxkX-3DHeEGc"},"source":["\n","# # Preprocessing to get the qestions and answers for train , test and val \n","# train = []\n","# test = []\n","# val = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"30zIOf_4e6EJ"},"source":["# paths\n","train_anno_path = \"/content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Annotated_images/v2_mscoco_train2014_annotations.json\"\n","val_anno_path =  \"/content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Annotated_images/v2_mscoco_val2014_annotations.json\"\n","\n","train_q_path = \"/content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Questions/v2_OpenEnded_mscoco_train2014_questions.json\" \n","val_q_path = \"/content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Questions/v2_OpenEnded_mscoco_val2014_questions.json\"\n","test_q_path = \"/content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/Questions/v2_OpenEnded_mscoco_test2015_questions.json\"\n","\n","save_anno_path = \"/content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/final_data/\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m2sHo2zheENy"},"source":["# # Loading annotations and questions...\n","# train_anno = json.load(open(train_anno_path, 'r'))\n","# val_anno = json.load(open(val_anno_path, 'r'))\n","\n","# train_ques = json.load(open(train_q_path, 'r'))\n","# val_ques = json.load(open(val_q_path, 'r'))\n","# test_que = json.load(open(test_q_path, 'r'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VXGDxHy_fxYF"},"source":["# # subtype = 'train2014'\n","# for i in range(len(train_anno['annotations'])):\n","#     ans = train_anno['annotations'][i]['multiple_choice_answer']\n","#     question_id = train_anno['annotations'][i]['question_id']\n","#     image_id = train_anno['annotations'][i]['image_id']\n","\n","#     question = train_ques['questions'][i]['question']\n","#     question_type = train_anno['annotations'][i]['question_type']\n","#     answers = train_anno['annotations'][i]['answers']\n","#     answer_type = train_anno['annotations'][i]['answer_type']\n","\n","#     train.append({'ques_id': question_id, 'img_id': image_id, 'question': question, 'ans': ans, 'question_type': question_type, 'answers': answers, 'answer_type': answer_type})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QvGhJjMgnbD_"},"source":["# # subtype = 'train2014'\n","# for i in range(len(val_anno['annotations'])):\n","#     ans = val_anno['annotations'][i]['multiple_choice_answer']\n","#     question_id = val_anno['annotations'][i]['question_id']\n","#     image_id = val_anno['annotations'][i]['image_id']\n","\n","#     question = val_ques['questions'][i]['question']\n","#     question_type = val_anno['annotations'][i]['question_type']\n","#     answers = val_anno['annotations'][i]['answers']\n","#     answer_type = val_anno['annotations'][i]['answer_type']\n","\n","#     val.append({'ques_id': question_id, 'img_id': image_id, 'question': question, 'ans': ans, 'question_type': question_type, 'answers': answers, 'answer_type': answer_type})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4CsTZlqvnbN8"},"source":["# # subtype = 'test2015'\n","# for i in range(len(test_que['questions'])):\n","#     question_id = test_que['questions'][i]['question_id']\n","#     image_id = test_que['questions'][i]['image_id']\n","#     question = test_que['questions'][i]['question']\n","\n","#     test.append({'ques_id': question_id, 'img_id': image_id, 'question': question})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"skCZFOyIydpX"},"source":["# saving files \n","\n","\n","# save(save_anno_path + \"train.pkl\" , train)\n","# save(save_anno_path + \"val.pkl\" , val)\n","# save(save_anno_path + \"test.pkl\" , test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PF184sKty-YV"},"source":["# train = load(save_anno_path + \"train.pkl\")\n","# val =  load(save_anno_path + \"val.pkl\")\n","# test = load(save_anno_path + \"test.pkl\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2kbB7SE5ctpy"},"source":["# https://github.com/Shivanshu-Gupta/Visual-Question-Answering/blob/master/preprocess.py"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_CG3fa_xeDP9"},"source":["# https://medium.com/@rajeshmane711/visual-question-answering-system-using-deep-learning-techniques-5636a9c6b72d"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8nrFBmTr-eFi"},"source":["## Extracting the features of images and save to disk "]},{"cell_type":"markdown","metadata":{"id":"bsnyVXANhrbv"},"source":["### Loaders and dataset and transforms "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KeixMulR-ghg","executionInfo":{"status":"ok","timestamp":1616120067773,"user_tz":-330,"elapsed":1342,"user":{"displayName":"Akanksha Shrimal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTgb81ryS4Z8HL8jjPWL6oDEg75JbHezBN4Gnk=s64","userId":"15584575180220244618"}},"outputId":"bf6d42cb-511b-4cfb-d58b-79dcc119031f"},"source":["# # Define transforms for the training data and testing data\n","\n","# train_transforms = transforms.Compose([transforms.RandomRotation(30),\n","#                                        transforms.RandomSizedCrop(224),\n","#                                        transforms.RandomHorizontalFlip(),\n","#                                        transforms.ToTensor(),\n","#                                        transforms.Normalize([0.485, 0.456, 0.406],\n","#                                                             [0.229, 0.224, 0.225])])\n","\n","# val_transforms = transforms.Compose([transforms.Scale(256),\n","#                                     transforms.CenterCrop(224),\n","#                                       transforms.ToTensor(),\n","#                                       transforms.Normalize([0.485, 0.456, 0.406],\n","#                                                            [0.229, 0.224, 0.225])])\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:886: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\n","  warnings.warn(\"The use of the transforms.RandomSizedCrop transform is deprecated, \" +\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:285: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n","  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"LlfVWHJ1A72X"},"source":["# # custom class to provide the image name along with the image \n","# class ImageFolderWithPaths(datasets.ImageFolder):\n","#     \"\"\"Custom dataset that includes image file paths. Extends\n","#     torchvision.datasets.ImageFolder\n","#     \"\"\"\n","\n","#     # override the __getitem__ method. this is the method that dataloader calls\n","#     def __getitem__(self, index):\n","#         # this is what ImageFolder normally returns \n","#         original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n","#         # the image name\n","#         img_name = self.imgs[index][0].split('/')[-1].split('_')[-1].lstrip(\"0\")\n","#         # print(path)\n","#         # make a new tuple that includes original and the path\n","#         tuple_with_path = (original_tuple + (img_name,))\n","#         return tuple_with_path"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FAa49YyZFrcO"},"source":["# instantiate the dataset and dataloader\n","\n","# Train \n","# root directory of train , test and val needs to be put here  \n","# train_data_dir = '/content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/tr_data'\n","# train_dataset = ImageFolderWithPaths(train_data_dir, transform=train_transforms) \n","# Val  \n","# val_data_dir = '/content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/v_data'\n","# val_dataset = ImageFolderWithPaths(val_data_dir, transform=val_transforms) \n","# # Test \n","# test_data_dir = \"/content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/te_data\"\n","# test_dataset = ImageFolderWithPaths(test_data_dir, transform=val_transforms) \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gV4nBsCUyjSK"},"source":["# Creating dataloaders for train test and val \n","# batch = 64\n","\n","# train_dataloader = torch.utils.data.DataLoader(train_dataset ,batch_size=batch )\n","# val_dataloader =  torch.utils.data.DataLoader(val_dataset ,batch_size=batch )\n","# test_dataloader =  torch.utils.data.DataLoader(test_dataset ,batch_size=batch )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RE0js6hJiZqh"},"source":["### Extractor "]},{"cell_type":"code","metadata":{"id":"m4WVKE1h1kwV"},"source":["# # Class to extract features from images \n","# class Encoder(nn.Module):\n","#     def __init__(self):\n","#         super(Encoder, self).__init__()\n","#         VGG = models.vgg16(pretrained=True)\n","#         self.feature = VGG.features\n","#         self.classifier = nn.Sequential(*list(VGG.classifier.children())[:-3])\n","#         pretrained_dict = VGG.state_dict()\n","#         model_dict = self.classifier.state_dict()\n","#         pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n","#         model_dict.update(pretrained_dict)\n","#         self.classifier.load_state_dict(model_dict)\n"," \n","#     def forward(self, x):\n","#         output = self.feature(x)\n","        \n","#         output = output.view(output.size(0), -1)\n","        \n","#         output = self.classifier(output)\n","      \n","#         return output\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Snkt374P3iEM"},"source":["# extractor = Encoder()\n","# print(extractor)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lxiVlZauCKyu"},"source":["# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","# print(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fghLRCsGicfL"},"source":["### Extracting features of train test and val "]},{"cell_type":"code","metadata":{"id":"27BW5doH3k1a"},"source":["\n","# extractor.to(device)\n","# saving_folder = [ 'val/']\n","# count = 0\n","# # [train_dataloader , val_dataloader, test_dataloader]\n","# for idx,loader in enumerate([val_dataloader]):\n","#   # labels got here are of no use \n","#   print(\"--------------------doing------------\" + saving_folder[idx] )\n","#   for inputs, labels, paths in loader:\n","#     # print(inputs, labels, paths)\n","\n","#     inputs = inputs.to(device)\n","#     feature_outputs = extractor(inputs)\n","\n","#     for f, p in zip(feature_outputs, paths):\n","#       count = count + 1\n","#       print(count)\n","#       np.save( '/content/drive/MyDrive/SUB/PROJECTS/Dl/processed_images/'+ saving_folder[idx] + p , f.cpu().detach().numpy())\n","#   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jb2xm2nkCQjQ"},"source":["\n","# extractor.to(device)\n","# saving_folder = ['test/']\n","# count = 0\n","# # [train_dataloader , val_dataloader, test_dataloader]\n","# for idx,loader in enumerate([ test_dataloader]):\n","#   # labels got here are of no use \n","#   print(\"--------------------doing------------\" + saving_folder[idx] )\n","#   for inputs, labels, paths in loader:\n","#     # print(inputs, labels, paths)\n","\n","#     inputs = inputs.to(device)\n","#     feature_outputs = extractor(inputs)\n","\n","#     for f, p in zip(feature_outputs, paths):\n","#       count = count + 1\n","#       print(count)\n","#       np.save( '/content/drive/MyDrive/SUB/PROJECTS/Dl/processed_images/'+ saving_folder[idx] + p , f.cpu().detach().numpy())\n","\n","      "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0_EkJkgvCvxW"},"source":["\n","# extractor.to(device)\n","# saving_folder = ['train/']\n","# count = 0\n","# # [train_dataloader , val_dataloader, test_dataloader]\n","# for idx,loader in enumerate([train_dataloader]):\n","#   # labels got here are of no use \n","#   print(\"--------------------doing------------\" + saving_folder[idx] )\n","#   for inputs, labels, paths in loader:\n","#     # print(inputs, labels, paths)\n","\n","#     inputs = inputs.to(device)\n","#     feature_outputs = extractor(inputs)\n","\n","#     for f, p in zip(feature_outputs, paths):\n","#       count = count + 1\n","#       print(count)\n","#       np.save( '/content/drive/MyDrive/SUB/PROJECTS/Dl/processed_images/'+ saving_folder[idx] + p , f.cpu().detach().numpy())\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_buQjFh6mM1Y"},"source":["# path1 = \"/content/drive/MyDrive/SUB/PROJECTS/Dl/processed_images/test/\"\n","# path2 = \"/content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/te_data/test/\"\n","# path3 = \"/content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/te_data2/test/\"\n","\n","# # # !google-drive-ocamlfuse -cc\n","# # # drive.flush_and_unmount()   \n","# !rm -r /content/drive/MyDrive/SUB/PROJECTS/Dl/processed_images/train2/"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Sckv8p2D0wj"},"source":["### Number of Images and Preprocessed number "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ED4jrctD4SA","executionInfo":{"status":"ok","timestamp":1616142397483,"user_tz":-330,"elapsed":3322,"user":{"displayName":"Akanksha Shrimal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTgb81ryS4Z8HL8jjPWL6oDEg75JbHezBN4Gnk=s64","userId":"15584575180220244618"}},"outputId":"be273849-a5bc-4def-c136-b78480f93a59"},"source":["!ls /content/drive/MyDrive/SUB/PROJECTS/Dl/processed_images/test -1 | wc -l\n","!ls /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/te_data/test -1 | wc -l\n","\n","!ls /content/drive/MyDrive/SUB/PROJECTS/Dl/processed_images/train -1 | wc -l\n","!ls /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/tr_data/train -1 | wc -l\n","\n","!ls /content/drive/MyDrive/SUB/PROJECTS/Dl/processed_images/val -1 | wc -l\n","!ls /content/drive/MyDrive/SUB/PROJECTS/Dl/Dataset/v_data/val -1 | wc -l\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["15998\n","15998\n","27763\n","40000\n","16438\n","16438\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SEt229Lel-Li"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gAeWHCgQCCu_"},"source":["# Main Class "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T5-jskSu5EdP","executionInfo":{"status":"ok","timestamp":1616218201120,"user_tz":-330,"elapsed":1216,"user":{"displayName":"Akanksha Shrimal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTgb81ryS4Z8HL8jjPWL6oDEg75JbHezBN4Gnk=s64","userId":"15584575180220244618"}},"outputId":"a3703ba4-75ac-472d-f8cb-71fcae5d7346"},"source":["# Device configuration \n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cpu\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WqvKQO9F6eem"},"source":["## Baseline-1"]},{"cell_type":"code","metadata":{"id":"I7asaTD1Hxc2"},"source":["def create_emb_layer(weights_matrix, non_trainable=False):\n","    num_embeddings, embedding_dim = weights_matrix.size()\n","    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n","    emb_layer.load_state_dict({'weight': weights_matrix})\n","    if non_trainable:\n","        emb_layer.weight.requires_grad = False\n","\n","    return emb_layer, num_embeddings, embedding_dim"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DJv6qQ4xCD8y"},"source":["#### 1000 answers data TYPE DATA ####\n","\n","from torch import nn, optim\n","import torch.nn.functional as F\n","class ImageModel(nn.Module):\n","    def __init__(self):\n","        super(ImageModel, self).__init__()\n","        self.fc1 = nn.Linear(4096, 1024)\n","        \n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = F.relu(x)\n","        return x\n","    \n","\n","class TextModel(nn.Module):\n","    def __init__(self, weights_matrix,hidden_size, num_layers):\n","        super(TextModel, self).__init__()\n","\n","        self.num_layers = num_layers\n","        self.hidden_size = hidden_size\n","        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True)\n","        self.embedding_dim = embedding_dim\n","        # -> x needs to be: (batch_size, seq, input_size)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n","        self.fc1 = nn.Linear(1536, 1024)\n","        \n","        \n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_size).to(device)\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","        \n","        x, _ = self.lstm(self.embedding(x), (h0,c0))  \n","        # take hidden value from only last time stamp \n","        x = x[:, -1, :]\n","        # make sure input tensor is flattened\n","        x = x.view(x.shape[0], -1)\n","        x =  F.relu(self.fc1(x))\n","      \n","        return x\n","\n","class CombinedModel(nn.Module):\n","    def __init__(self, ImageModel, TextModel):\n","        super(CombinedModel, self).__init__()\n","        self.ImageModel = ImageModel\n","        self.TextModel = TextModel\n","\n","        self.bn1 = nn.BatchNorm2d(num_features=1024)\n","        self.drop = nn.Dropout(p=0.5)\n","        self.fc1 = nn.Linear(1024,1000)\n","        \n","        \n","    def forward(self, x1, x2):\n","        x1 = self.ImageModel(x1)\n","        x2 = self.TextModel(x2)\n","        # element wise multiplication \n","        x3 = torch.mul(x1,x2)\n","        x3 = self.bn1(x3)\n","        x3 = self.drop(x3)\n","        \n","        output = F.log_softmax(self.fc1(x3), dim=1)\n","        \n","        return output\n","\n","# # Create models and load state_dicts    \n","# image_ml = ImageModel().to(device)\n","# text_ml = TextModel(64,2).to(device)\n","\n","# # Load state dicts\n","# # image_ml.load_state_dict(torch.load(PATH))\n","# # text_ml.load_state_dict(torch.load(PATH))\n","\n","# model = CombinedModel(image_ml, text_ml).to(device)\n","# criterion = nn.NLLLoss()\n","# optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","#### STORE THE MODEL ALONG WITH STATE CODE #####\n","#### MOVE MODEL TO GPU CODE ####\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oc3IWRz9H7Fm"},"source":["# #### YES / NO TYPE DATA ####\n","\n","# from torch import nn, optim\n","# import torch.nn.functional as F\n","# class ImageModel(nn.Module):\n","#     def __init__(self):\n","#         super(ImageModel, self).__init__()\n","#         self.fc1 = nn.Linear(4096, 1024)\n","        \n","#     def forward(self, x):\n","#         x = self.fc1(x)\n","#         x = F.relu(x)\n","#         return x\n","    \n","\n","# class TextModel(nn.Module):\n","#     def __init__(self,hidden_size, num_layers,embedding_dim=300):\n","#         super(TextModel, self).__init__()\n","#         self.num_layers = num_layers\n","#         self.hidden_size = hidden_size\n","#         self.embedding_dim = embedding_dim\n","#         # -> x needs to be: (batch_size, seq, input_size)\n","#         self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n","#         self.fc1 = nn.Linear(1536, 1024)\n","        \n","        \n","#     def forward(self, x):\n","#         h0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_size).to(device)\n","#         c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","\n","#         x, _ = self.lstm(x, (h0,c0))  \n","#         # take hidden value from only last time stamp \n","#         x = x[:, -1, :]\n","#         # make sure input tensor is flattened\n","#         x = x.view(x.shape[0], -1)\n","#         x =  F.relu(self.fc1(x))\n","      \n","#         return x\n","\n","# class CombinedModel(nn.Module):\n","#     def __init__(self, ImageModel, TextModel):\n","#         super(CombinedModel, self).__init__()\n","#         self.ImageModel = ImageModel\n","#         self.TextModel = TextModel\n","\n","#         self.bn1 = nn.BatchNorm2d(num_features=1024)\n","#         self.drop = nn.Dropout(p=0.5)\n","#         self.fc1 = nn.Linear(1024,1000)\n","#         self.classifier = nn.Linear(1000, 2)\n","        \n","#     def forward(self, x1, x2):\n","#         x1 = self.ImageModel(x1)\n","#         x2 = self.TextModel(x2)\n","#         # element wise multiplication \n","#         x3 = torch.mul(x1,x2)\n","#         x3 = self.bn1(x3)\n","#         x3 = self.drop(x3)\n","#         x3 = F.relu(self.fc1(x3))\n","#         output = F.log_softmax(self.classifier(x3), dim=1)\n","        \n","#         return output\n","\n","# # Create models and load state_dicts    \n","# image_ml = ImageModel().to(device)\n","# text_ml = TextModel(64,2).to(device)\n","\n","# # Load state dicts\n","# # image_ml.load_state_dict(torch.load(PATH))\n","# # text_ml.load_state_dict(torch.load(PATH))\n","\n","# model = CombinedModel(image_ml, text_ml).to(device)\n","# criterion = nn.NLLLoss()\n","# optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# #### STORE THE MODEL ALONG WITH STATE CODE #####\n","# #### MOVE MODEL TO GPU CODE ####\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g0lDfzCS6i5h"},"source":["## Baseline-2"]},{"cell_type":"code","metadata":{"id":"bEU1oa1RIe6c"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CEopppV2yUSK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d35B2eDzF06I"},"source":["# Load preprocessed question from here \n","class Get_Image_Que(Dataset):\n","    \"\"\" dataset.\"\"\"\n","\n","    def __init__(self, data_file , root_dir, transform=None):\n","      # data file is a pandas dataframe containing all image ids and all question ids along with question and answers \n","        \n","        self.data_file = data_file\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.img_ids = data_file['img_id'].to_numpy()\n","        self.que_ids = data_file['ques_id'].to_numpy()\n","\n","    def __len__(self):\n","        return len(self.data_file)\n","\n","    def __getitem__(self, idx):\n","\n","        # loading numpy preprocessed features of image \n","        temp = str(self.img_ids[idx]) + \".jpg\"\n","        img_name = os.path.join(self.root_dir,\n","                               \"142.jpg\" + \".npy\")\n","        img_tensor = torch.tensor(np.load(img_name))\n","\n","        ############## HARD CODED CHANGE IMAGE FILE NAME #########\n","\n","        # loading the question \n","        curr_que_id = self.que_ids[idx]\n","        curr_que = self.data_file[self.data_file['ques_id'] == curr_que_id]['question'].to_numpy()[0]\n","        curr_ans = self.data_file[self.data_file['ques_id'] == curr_que_id]['ans'].to_numpy()[0]\n","        \n","        if self.transform:\n","            # sample = self.transform(sample)\n","            pass\n","\n","        return (img_tensor,curr_que,curr_ans)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qa3v9C38F1By"},"source":["# train_data = Get_Image_Que(df, \"/content/drive/MyDrive/SUB/PROJECTS/Dl/processed_images/val/\")\n","# train_loader = DataLoader(train_data, batch_size=2, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MwBaUkLdF7QB"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5-On1SPsQKd8"},"source":["## Training loop "]},{"cell_type":"code","metadata":{"id":"zdT3QlqbQL8M"},"source":["epochs = 3\n","steps = 0\n","\n","train_losses, test_losses = [], []\n","for e in range(epochs):\n","    running_loss = 0\n","    for images, questions, labels in train_loader:\n","\n","        # tokenize the question array \n","\n","        # images and labels and toknize output sent to device \n","        images, labels = torch.reshape(images,(-1,sequence_length))  , labels.to(device) \n","        images = func(images)\n","        images = torch.tensor(images, dtype=torch.long).to(device) \n","        \n","        \n","        optimizer.zero_grad()\n","        \n","        log_ps = model(images)\n","        loss = criterion(log_ps, labels)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        running_loss += loss.item()\n","        \n","    else:\n","        test_loss = 0\n","        accuracy = 0\n","        \n","        # Turn off gradients for validation, saves memory and computations\n","        with torch.no_grad():\n","            model.eval()\n","            for images, questions, labels in test_loader:\n","              \n","                images, labels = torch.reshape(images,(-1,sequence_length))  , labels.to(device) \n","                images = func(images)\n","                images = torch.tensor(images, dtype=torch.long).to(device) \n","\n","                log_ps = model(images)\n","                test_loss += criterion(log_ps, labels)\n","                \n","                ps = torch.exp(log_ps)\n","                top_p, top_class = ps.topk(1, dim=1)\n","                equals = top_class == labels.view(*top_class.shape)\n","                accuracy += torch.mean(equals.type(torch.FloatTensor))\n","        \n","        model.train()\n","        \n","        train_losses.append(running_loss/len(train_loader))\n","        test_losses.append(test_loss/len(test_loader))\n","\n","        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n","              \"Training Loss: {:.3f}.. \".format(train_losses[-1]),\n","              \"Test Loss: {:.3f}.. \".format(test_losses[-1]),\n","              \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))\n","        \n","        if e%10 == 0:\n","          # print(model.lstm.weight_ih_l1)\n","          print(model.embedding.weight)"],"execution_count":null,"outputs":[]}]}